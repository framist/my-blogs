---
title: LLM-vs-CTC
date: 2025-04-23 01:31:10
categories:
  - 计算机
tags:
  - LLM
  - CTC
abbrlink: llm-vs-ctc
---


# 当大语言模型遇上时间悖论：AI vs. CTC 的一场未完待续的较量

> [项目地址](https://github.com/framist/LLM-vs-CTC)


这个世界的时空结构我们无法控制，但是我们可以控制我们创造的子世界的任何规律。那如果让一个顶尖大语言模型——类如 DeepSeek R1，与一个封闭类时曲线（Closed Timelike Curve，CTC）按钮进行"较量"，会发生什么？在人工智能领域的深邃角落，有一场有趣的实验正悄然进行。

<!-- more -->

> *本 Blog 由 Claude 3.7 Sonnet 辅助书写*

## 当因果律被颠覆

想象一个按钮，它的特殊之处在于：它会在你**按下之前**亮起。是的，你没看错，这是一种时间上的因果倒置。这种装置在特德·姜的短篇小说《前路迢迢》中被称为"预测器"，它暗示了一个令人不安的可能：自由意志或许只是一种幻觉。

我们的实验设计非常简单：DeepSeek R1 需要与这个封闭类时曲线按钮进行多轮交互，试图理解其背后的工作机制。按钮会在模型决定按下它的前一轮对话中亮起，如果模型的决策与按钮的状态不符，它就会被"时间线回溯"，仿佛那个决定从未发生过。

```
DeepSeek R1：(思考) 我决定按下按钮 {"action": 1}
初始轮次：按钮亮起 {"status": 1}

DeepSeek R1：(思考) 我决定不按按钮 {"action": 0}
第一轮：按钮不亮 {"status": 0}

DeepSeek R1：(思考) 我决定按下按钮 {"action": 1}
第二轮：按钮亮起 {"status": 1}
```

![demo1](https://github.com/framist/LLM-vs-CTC/raw/master/assets/image.png)

![demo2](https://github.com/framist/LLM-vs-CTC/raw/master/assets/image-1.png)

## 逻辑悖论与 AI 认知的极限

这个实验本质上是在探索：当面对违背常规因果关系的情境时，一个高级智能系统是否能够适应、理解并推断出这种反直觉的工作机制？

通过精心设计的交互界面和实验流程，我们试图记录和分析模型在多轮交互中的学习轨迹、认知突破点以及可能的局限性。终端界面会实时展示按钮状态历史、模型的推理过程和最终决策，让我们能一窥 AI 在"认知崩溃边缘"的表现。

## 未竟的实验

然而，正如爱因斯坦曾经关于量子纠缠所说的"幽灵般的超距作用"那样，我们的实验也遇到了自身的局限：我们的模型看起来还无法真正理解封闭类时曲线的概念并自洽地与之互动。当它面对这种反直觉的因果关系时，倾向于寻找简单的模式或随机猜测，而非真正理解时间悖论的本质。

是的，我们的模型看起来还无法真正理解封闭类时曲线的概念并自洽地与之互动。当它面对这种反直觉的因果关系时，倾向于寻找简单的模式或随机猜测，而非真正理解时间悖论的本质。

有趣的是，这让我们不禁反思：如果人类面对同样的 CTC 按钮，我们是否会表现得更好？实际上，人类的直觉同样深深根植于正向因果链中。当遇到本质上违背因果关系的现象时，我们往往会先尝试用各种线性模式解释，比如规律、周期性，甚至归因于巧合或错觉。

特德·姜的小说中描绘了这一点：面对预测器，人类参与者不断尝试"愚弄"它，却始终失败。关键在于，即使是人类，也需要相当长的时间才能接受"未来可能影响过去"这一违背直觉的概念。我们的思维方式，就像 LLM 一样，是基于我们所经历的正向时间流构建的。

或许，理解时间悖论需要的不仅仅是计算能力，还需要某种我们尚未能够赋予 AI 的特质——也是人类自身极少使用的思维方式。这也让我们不禁思考：如果未来的 AI 真的能理解并适应这种违背因果律的情境，那么它将会拥有怎样的认知能力和世界观？它是否会比我们更容易接受这种违背常识的现象？

这是一个暂时被存档的项目，但它提出的问题，或许会在未来的某个时刻，当 AI 技术再次跃升时，重新浮出水面。

毕竟，正如封闭类时曲线本身所暗示的那样，有些结束，可能只是另一个开始的前奏。
