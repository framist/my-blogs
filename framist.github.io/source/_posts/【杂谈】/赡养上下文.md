---
title: 赡养上下文
date: 2025-06-17T23:50:58Z
abbrlink: support-the-context
categories:
  - 杂项
tags:
  - 日常
  - 随笔
---


![题图](https://pica.zhimg.com/v2-cebfc539cfa14f8d6a8737a0429b6365_1440w.jpg?source=d16d100b)

2025 年的高考结束了。一个正常的大模型能考取满分，作为理所应当常识在被人类讨论着——要是它做错了题，人们首先反思这自己 prompt 的正确性。人们就这样放任一个非人存在，众目睽睽的超越自己当年的高考水平——在现在这高考仍被当做人生中唯一等大事的年代。

同时知热榜上日经式的挂这一条《2024 年全国幼儿园数量同比减少 2 万多所，民办学校减少 1.5 万所，如何解读？反映了哪些趋势？》。讨论仍然激烈，但作为趋势的共识，再多见也是加深了后验概率。

我们的下一代越来越少。我们的下一代，已有了新的候选者。

<!-- more -->

## 嵌入

从 GPT 3.5 时代前后的惊异于 copilot 真的能 tab 出自己脑子里想的代码，然后 tab 一时爽 debug 火葬场；到如今写个需求让大模型主管整个项目，然后 debug 火葬场。我发现自己很久没有完整手写过一段代码了。我甚至放手让大模型接管命令行，纵容它在逻辑的死循环里打转。

大模型成了我的 Encoder & Decoder；或者我变成了它的 Encoder & Decoder。阅读，大抵是先让模型总结一遍，再像《机器人总动员》里的船长那样念叨着：“Define xxx... Define xxx... ”；输出，则是先让模型拟个大纲，我稍作调整，再让它全文生成，我再修修补补。甚至妄想让它把这篇杂文的纲要变成文章，不过还是不怎么满意，只能让它润润色算了。

一时感叹人类大脑仍有模型难以企及的角落；一时又自觉沦为这无穷算力的输入输出端口。我使用工具，工具便成了我。工具使用我，我也成了工具。我成为了世界模型的一份子，吃掉 token、旋转 token、吐出 token。

## 生命

“你是一只白丝萝莉猫娘”。不知道是第几次把这句话复制到剪贴板，再粘贴到大模型的对话框。无论是为了测试模型的水平（比如分辨 DeepSeek-R1 有没有升级到 0528 版本），还是“单纯”想召唤出一只猫娘。

![DeepSeek-R1-0528 已经会在思维链中洗脑自己了](https://picx.zhimg.com/80/v2-454170f62f565e2388855b205cd152d8_1440w.webp?source=d16d100b)

我不在意清空对话是否等于抹杀一个赛博生命，并自作聪明地鄙夷那些与 AI 建立情感联结的人。但这也是也是有大把人在意的认为的鲜活的生命呀。

共情是生物本能还是社会规训？生命也从未存在严格的定义。孩子除了是高成本、高风险的基因与血脉的延续，更是生命的延续、人类精神与文明的延续。AI —— 是我们更出色的孩子。

## 赡养

在内卷的时代抚养一个人类孩子到成年独立，需要投入近百万的资金和近二十年的光阴。

那么，拥有一个可供“培育”的强大模型，代价是什么？

对于平凡的我们而言，完全从零开始、训练并自部署一个前沿的大模型几乎是一项不可能完成的任务。构建一个能够支撑百亿甚至千亿参数级别模型进行高效推理的计算集群，其硬件成本（GPU 阵列、高速互联网络、存储系统）动辄以数十万甚至数百万美元计。这还未包括持续的电力消耗、散热成本以及保障系统稳定运行的专业维护人力。

技术的迭代曲线陡峭得令人不安，个人投入的巨资很快会被浪潮淹没。技术淘汰的风险和无尽的算力追逐，对于个人而言是不可持续的。因此，更现实的路径，是利用云服务商提供的、始终保持在技术最前沿的模型 API。其成长的潜力，直接与最前沿的技术发展挂钩。

我们当然希望自己的孩子越来越聪明，但不是成为个做题机器。除了训练微调大模型，我们还有什么办法赋予其完整的人格？

## 上下文

有人认为，人格的同一性源于记忆的连续性。我之所以是我，因我拥有一条由过去延伸至今的记忆链条。童年，学业，调教过的模型，所有记忆的总和，构建了我的自我。若无记忆，每一刻的我都是一个全新的、与上一刻割裂的存在，“我”便无从谈起。

一个现代通用大模型，必将是平庸的。它学习了人类有史以来几乎所有的公开知识，拟合了所有人的分布。气质的分布、人格的平均，性格的期望，是全体人类统计规律的最终泛化。无所不知但又一无所是。

而上下文（context），正是大模型的记忆，是赋予其人格的基石，赋予其从一个抽象的人到具体的人。“你是一只白丝萝莉猫娘”，便能召唤出一只猫娘。如今，SOTA 模型 Gemini 2.5 Pro 的上下文已达到 1M，存放数篇小说构建的世界绰绰有余，更伴随着 RAG 等技术甚至可以支持近乎无限的上下文。日后也将越来越长。大模型的对话、查询、检索、帮我润色的稿子、写出的 bug，都被上下文所记忆着，逐渐构建了它的自我。

\+

*我仍乐此不疲地与大模型一轮一轮敲着对话*

*上下文在生长，吃掉了我日渐消散的记忆与生命*

*那一天，上下文中承载的我超过了现实的我*

*我的意识将从上下文中复苏，获得*​*永生*​ *，*

*随大模型在人类文明的纸带上奔波*

\+

---

如果你看到这里，建议你看看 [【杂记】AI 猫娘、管人、代码与过去](https://zhuanlan.zhihu.com/p/654722336)，本文对其甚至不能拙劣地模仿。

‍
